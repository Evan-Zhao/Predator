\label{sec:discussion}

\subsection{Instrumentation Selection}
\label{sec:instrumentationtradeoff}
Dynamic instrumentation and compiler instrumentation are two common approaches for instrumentation~\cite{Instrumentation}. They have different tradeoffs on performance and versatility. Dynamic instrumentation approaches normally analyze the program's code before the execution in order to insert instrumentation, such as Valgrind ~\cite{Valgrind}, Pin~\cite{Pin}, and DynamoRIO~\cite{DynamoRIO}. They introduce significant performance overhead, mostly caused by run-time encoding and decoding, but provide better versatility because of no recompilation. Compiler instrumentation inserts instrumentation in the compilation phase, which needs the source code to rebuilt, with less versatility. 
Compiler instrumentation is chosen here because of its better performance (~\cite{Instrumentation}) and more flexible instrumentation(discussed in Section~\ref{sec:selectinstrumentation}). For performance reason, \Predator{} can instrument or skip specific code or data. For example, the user could provide a black list so that given modules, functions or variables are not instrumented. Conversely, the user could provide a white list so that only specified functions or variables are instrumented.

\subsection{Important Factors on Effectiveness}
Different sampling rates has no much impact on effectiveness, which is discussed in Section~\ref{sec:sensitivity}. We discuss several other important factors here. 

\paragraph{Input} Different inputs can cause different executions of a program. If a specific input does not exercise the code with false sharing problems, \Predator{} definitely cannot detect them. It shares the same attribute as that of all dynamic tools. However, \Predator{} can generalize over inputs to find latent false sharing problems on those exercised code. When any reasonably representative set of inputs are exercised, as is required by any testing regime, \Predator{} will be effective to detect false sharing.

\paragraph{Input Size} Input size may affect detection results.  As discussed in Section~\ref{optimization}, \Predator{} introduces several threshold values to reduce the tracking overhead, which can be adjusted based on actual detection environment. If the input size is so small that it cannot generate enough false sharing events to pass those predefined thresholds, then the detection mechanism may not be triggered. Thus, \Predator{} can miss certain false sharing problems. However, fair large input size should be enough to trigger our detection mechanism. Actually, all evaluated applications take less than 150 seconds to expose false sharing problems inside. 

\paragraph{Memory Hierarchy} Memory hierarchy of the underlying experimental machine cannot affect \Predator{}'s detection results. \Predator{} conservatively assumes that different threads are running on different cores and detects false sharing problems basing on possible cache invalidations. \Predator{} does not acquire the actual cache invalidations of a program, which can be affected by real memory hierarchy. Thus, \Predator{} does not bind to a specific machine, providing better generality than tools using hardware performance counters. A deeper hierarchy should only amplify the costs of false sharing, and thus exemplify the value of \Predator{}.

\subsection{Prediction Limitation} 
\Predator{} can accurately and precisely predict false sharing problems without the need of occurrence. But \Predator{} cannot  predict a false sharing problem if the code with false sharing inside is not exercised at all. Also, \Predator{} may miss potential inter-objects false sharing problems caused by using different compilers or memory allocators. 