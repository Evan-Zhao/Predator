\label{sec:discussion}

\subsection{Instrumentation Selection}
\label{sec:instrumentationtradeoff}
Dynamic instrumentation and compiler instrumentation are two different approaches to do instrumentation~\cite{Instrumentation}. They have different tradeoffs on performance and versatility. Dynamic instrumentation normally analyzes the program's code before the execution in order to insert instrumentation, such as Valgrind ~\cite{Valgrind}, Pin~\cite{Pin}, and DynamoRIO~\cite{DynamoRIO}. They introduce significant performance overhead, mostly caused by run-time encoding and decoding, but provide better versatility because there is no need to recompile the code. Compiler instrumentation inserts instrumentation in the compilation phase, which needs the source code to rebuilt, with less versatility. 
Compiler instrumentation is chosen here because of its better performance (~\cite{Instrumentation}) and more flexible instrumentation(discussed in Section~\ref{sec:selectinstrumentation}). For performance reason, \Predator{} can instrument or skip specific code or data. For example, the user could provide a black list so that given modules, functions or variables are not instrumented. Conversely, the user could provide a white list so that only specified functions or variables are instrumented.

\subsection{Important Factors}
The effect of sampling rate has been evaluated in Section~\ref{sec:sensitivity}.  We describes several important factors which may or may not affect detection results here. 

\paragraph{Input} Different inputs cause different executions of a program. If a specific input does not exercise the code with false sharing problems, \Predator{} can not detect them, which is the same as all dynamic tools. Because it is predictive, \Predator{} can generalize over inputs to find latent false sharing problems on those exercised code. We expect that when any reasonably representative set of inputs are exercised, as is required by any testing regime, \Predator{} will be effective to detect false sharing.

\paragraph{Input Size} Input size may affect detection results. If input is too small to pass our predefined threshold value, therefore the false sharing detection mechanism can not be triggered and certain false sharing can be missed. However, fair large input size should be enough to trigger our detection mechanism. Actually, all evaluated applications take less than 150 seconds to expose specific false sharing problems. In addition, these threshold values can be adjusted based on actual detection environment.

\paragraph{Memory Hierarchy} Real memory hierarchy of experimental machine can not affect \Predator{}'s detection results. \Predator{} conservatively assumes that different threads are running on different cores and detects problems based on possible cache invalidation. \Predator{} does not acquire the actual cache invalidations of a program, which can be affected by real memory hierarchy of the underlying experimental machine. Thus, \Predator{} does not bind to a specific machine, providing better generality than tools using hardware performance counters. A deeper hierarchy should only amplify the costs of false sharing, and thus exemplify the value of \Predator{}.

\subsection{Prediction Limitation} 
\Predator{} can accurately and precisely predict false sharing problems without the need of occurrence. But \Predator{} cannot  predict a false sharing problem if the code with false sharing inside is not exercised at all. Also, \Predator{} may miss potential inter-objects false sharing problems of using different compilers or memory allocators if those objects are not placed in adjacent cache lines in the detection. For example, two objects $A$ and $B$, which are not placed in adjacent cache lines, can be placed in adjacent cache lines when using a different compiler or memory allocator. \Predator{} can not predict this type of false sharing. 