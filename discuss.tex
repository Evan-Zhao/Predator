\label{sec:discussion}

\subsection{Instrumentation Selection}
\label{sec:instrumentationtradeoff}
Dynamic instrumentation and compiler instrumentation are two common approaches for instrumentation~\cite{Instrumentation}. They have different tradeoffs on performance and generality. Dynamic instrumentation approaches, such as Valgrind ~\cite{Valgrind}, Pin~\cite{Pin}, and DynamoRIO~\cite{DynamoRIO}, normally analyze the program's code before the execution in order to insert instrumentation. They introduce significant performance overhead, mostly caused by run-time encoding and decoding, but provide better generality because of no recompilation. Compiler instrumentation inserts instrumentation in the compilation phase, which needs the source code to be rebuilt, with less generality. 
Compiler instrumentation is chosen here because of its better performance (~\cite{Instrumentation}) and more flexible instrumentation (discussed in Section~\ref{sec:selectinstrumentation}). For performance reason, \Predator{} can instrument or skip specific code or data. For example, the user could provide a black-list so that given modules, functions or variables are not instrumented. Conversely, the user could provide a white-list so that only specified functions or variables are instrumented.

\subsection{Important Factors on Effectiveness}
Different sampling rates do not affect effectiveness, which is discussed in Section~\ref{sec:sensitivity}. We discuss several other important factors here. 

\emph{Input} Different inputs can cause different executions of a program. If a specific input does not exercise the code with false sharing problems, \Predator{} definitely cannot detect them, sharing the same attribute as that of all dynamic tools. However, \Predator{} does generalize over inputs to find latent false sharing problems on those exercised code. When any reasonably representative set of inputs are exercised, as is required by any testing regime, \Predator{} can effectively detect false sharing.

\ emph{Input Size} Input size may affect detection results.  As discussed in Section~\ref{optimization}, \Predator{} introduces several threshold values to reduce the tracking overhead, which can be adjusted based on actual detection environment. If the input size is so small that it cannot generate enough false sharing events to pass those predefined thresholds, then the detection mechanism may not be triggered. Thus, \Predator{} can miss certain false sharing problems. However, fair large input size should be enough to trigger our detection mechanism. Actually, all evaluated applications take less than 150 seconds to expose false sharing problems inside. 

\ emph{Memory Hierarchy} Memory hierarchy of the underlying experimental machine cannot affect \Predator{}'s detection results. \Predator{} conservatively assumes that different threads are running on different cores and detects false sharing problems based on possible cache invalidations. \Predator{} does not attempt to obtain the actual cache invalidations of a program, which can depend on real memory hierarchy. Thus, \Predator{} does not bind to a specific machine, providing better generality than tools using hardware performance counters. A deeper hierarchy should only amplify the costs of false sharing, and thus exemplify the value of \Predator{}.

\subsection{Prediction Limitation} 
\Predator{} can accurately and precisely predict a false sharing problem even when it does not occur. But \Predator{} cannot predict a false sharing problem if the code with false sharing is not exercised at all. Also, \Predator{} may miss potential false sharing problems between two objects brought by a different compiler or memory allocator. 