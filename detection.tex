\subsection{Overview}
%\defaults{} is a hybrid approach that combines the runtime system with compiler instrumentation.
\label{sec:overview}
%false sharing is caused by cache coherence protocol: 
%when data of a cache line on a core is changed, the duplicated data of the same cache line 
%in any other core must be invalidated in order to guarantee the correctness.
Frequent cache invalidations caused by false sharing can cause severe performance problem.
To better describe the problem and our approach, we assume each thread runs on a distinct core throughout this
paper. Given this assumption, we observe that 
\textbf{if a thread writes a cache line after other threads have 
accessed the same cache line, this write operation most likely causes at least
one invalidation}. 
Based on this observation, \Defaults{} captures cache invalidations for all cache lines and 
identify performance-degradating false sharing in source code using this information.
% by keeping track of accesses from different threads. 
 
%to be loaded from main memory, wasting both the CPU time and memory bandwidth in the same time.
To capture cache invalidations, it is important to track memory accesses from different 
threads. 
Without hardware, however, it is difficult for runtime system itself to capture read and write accesses efficiently, 
accurately and timely. 
For example, \Sheriff{} relying on memory protection mechanism and word-by-word
comparison to track accumulated write accesses in a period of time 
entails XXX overhead~\cite{sheriff}. 

%Liu et al. using one
%tool of binary 
%instrumentation (Pin) to collect memory accesses introduces more than 
%$100\times$ performance overhead ~\cite{falseshare:binaryinstrumentation2}   

On the other hand, compiler can easily identify  
read or write accesses by looking at instruction types. However,
it is lack of the knowledge about when and how those instructions are being accessed, 
which depend on a specific execution including input and runtime environment. 
%Fortunately, runtime system can capture these kinds of information with the help of compiler isntrumentation. 

Therefore, \defaults{} combines the runtime system with compiler instrumentation technique to capture 
cache invalidations: compiler instruments memory accesses so the runtime
system is notified when an access is executed (see Section~\ref{sec:compiler}),
and the runtime system is responsible for collecting and analyzing actual memory accesses 
to detect and report false sharing (see Section~\ref{sec:runtime}).

\subsection{Compiler Instrumentation}
\label{sec:compiler}

\Defaults{} relies on LLVM to perform instrumentation on Intermediate Representation level (IR)~\cite{llvm}.
It traverses all functions one by one and 
searches for memory accesses on global and heap variables. 
For each memory access, \defaults{} adds one function call to jump into the runtime system with memory access  
address and access type.
The reason \Defaults{} skip accesses to stack variables 
is that stack variables are normally used for be thread local storage and
therefore do not introduce false sharing. However, instrumentation on stack
variables can always be turned on when necessary. 

The instrumentation pass is placed at the very end of LLVM optimization passes 
so that only those memory accesses surviving all previous 
LLVM optimization are instrumented. This 
technique is very similar to the one used in AddressSanitizer~\cite{Addresssanitizer}.
%But this technique could be very costly and we discuss the optimization in the Section~\ref{optimization}. 

\subsection{Runtime System}
\label{sec:runtime}

Runtime system tracks every executed memory access by handling 
information passed through by functions calls inserted during compiler
instrumentation phase. 
%Same as \Sheriff{}, 
%\Defaults{} reports precise information about a false sharing problem.
By analyzing traces of memory accesses, \defaults{} precisely reports 
performance-degrading false sharing in the end.
Specifically, for each global variable invovled in false sharing, its name, address and size
information are reported. 
For heap objects, actual callsite stack for allocations, address 
and size information are reported. 
In addition, \defaults{} provides word accesses information for those cache
lines involved in false sharing, 
including which threads are accessing which words. This information can help 
users locate where the problem is and how to fix false sharing problem.

\subsubsection{Detecting Cache Invalidations}
\Defaults{} reports those global variables or heap objects on those cache lines 
having a large amount of cache invalidations. 
Thus, it is critical to capture invalidation information effectively. 
\Defaults{} achieves this goal by introducing a  
two-entries-cache-history table for each cache line, as shown in Table~\ref{table:cachehistory}. 

\begin{table}
\centering
  \begin{tabular}{ l | r }
    \hline
    {Thread ID} & {Type of Access} \\ \hline
    \hline
     &   \\ \hline
     &   \\ \hline
  \end{tabular}
  \caption{Two-entries-cache-history table. \label{table:cachehistory}}
\end{table} 

In this table, each entry has two fields: thread ID and access type (read or write). 
For every new access on a cache line {\it l}, we check {\it l}'s history table
{\it T} to decide whether there is a cache invidation based on following rules:

\begin{itemize}
\item
  For a read access {\it R}, 
  \begin{itemize}
    \item
      If {\it T} is full, there is no need to record this read access, becasue
      read accesses cannot generate cache invalidations.
    \item
      If {\it T} is empty, or not full and another existing entry has a different thread
      ID, then we record {\it R} and its thread by filling a new entry in to the table. 
  \end{itemize}
\item
  For a write access {\it W}, 
  \begin{itemize}
    \item
      If {\it T} is full, then {\it W} can cause a cache invalidation since at least 
      one of two existing entries has a different thread ID for this cache line. 
    \item
      If {\it T} is empty,
      then we record {\it W} and its thread by filling a new entry in to the table. 
    \item
      If {\it T} has an exisiting entry,
      we check if {\it W} and the exisiting entry has the same thread ID. If
      so, {\it W} can not cause a cache invalidation, so we update the existing
      entry with {\it W}. Otherwise, we identify an invalidation on this line caused by
      {\it W}. After recording this invalidation information, we update 
      existing entry with {\it W} and its thread.
  \end{itemize}
\end{itemize}

\subsubsection{Reporting False Sharing}
After those individual cache lines having a large amount of cache
invalidations are detected,
\defaults{} needs further analysis to identify actual false sharing. 
This is because a cache line having a large amount of cache invalidations does not 
always mean false sharing. 
True sharing such as mulitple threads updating 
the same counter in a cache line also causes invalidations.

In order to report false sharing precisely and accurately, 
\Defaults{} explores the following mechanisms. 
\begin{itemize}
\item
\defaults{} keeps track of access information for each word on those
cache lines involved in false sharing: how many reads or writes to each word by which thread. 
When a word is accessed by multiple threads,
we marked the access as a shared access. This information 
allows \defaults{} to accurately distinguish false sharing from true sharing 
in the reporting phase,  
It also helpful to locate where 
actual false sharing occurs when there are multiple fields or multiple objects in the same cache line. 
\textbf{EXPLAINING TWO TYPES OF FALSE SHARINGS}

\item
In order to precisely report origins of false sharing objects, \defaults{}
keeps callsite information for each heap object and reports the source code level
information about each heap object. To obtain those callsite information, \defaults{}
intercepts all memory allocations and de-allocations,  and relies on \texttt{backtrace()} 
function to obtain the callsite stack. 
\defaults{} also avoids pseudo false sharing (false positives) caused by memory re-usages 
by handling correspondingly in memory de-allocations, whereas those heap objects with false 
sharing is not re-used at all.

\item
To keep tracking of reads and writes on each word for those problematic cache lines,
the bookkeeping information lookup must be efficient.
Similar to 
AddressSanitizer~\cite{Addresssanitizer} and other systems~\cite{qinzhaodetection}~\cite{Valgrind}. 
\defaults{} uses shadow memory mechanism
where each cache line of user space has a corresponding entry in the shadow memory. 
Based on the address of incoming access, it can directly  
the metadata for the corresponding cache line.


\item
To map user space of applications, \defaults{} uses predefined starting address and fixed size for 
heap. It also contains a customized memory allocator, which is built on 
Heaplayers~\cite{heaplayers} based on a ``per-thread-heap'' mechanism firstly introduced 
by Hoard~\cite{Hoard}. Memory allocations from different threads 
are not coming from the same physical cache line, which automatically avoids false sharing 
introduced by memory allocator.
However, it also brings a shortcoming that those false sharings caused by the default memory 
allocator can not be detected by \defaults{}. 

\end{itemize} 
 
\subsubsection{Threshold-Based Tracking Mechanism}
\Defaults{} aims to detect performance degrading false sharing.
% where those cache lines with a large amount of cache invalidations and writes.
Since cache invalidations are the root cause of performance degrading and only writes 
can possibly introduce cache invalidations, 
those cache lines with a small amount of writes are never being our target.
For this reason, \defaults{} starts to track cache invalidations, reads and word accesses 
of a cache line only after the number of writes on this cache line is above a
pre-defined threshold, called {\it Tracking-Threshould}. 
Before this threshold is reached, \defaults{} only tracks writes on a cache line while skipping those 
reads operations. 
This mechanism helps to filter out
those impossible candidates, reduce memory overhead and optimize the overall performance. 

In our implementation, \defaults{} maintains two mappings in shadow memory: 
{\it CacheWrites}, which tracks memory writes on every cache line, 
and {\it CacheTrackings}, which tracks detailed information 
on each cache line when the number of writes on a cache line is larger than
the {\it Tracking-Threshould}. 
If the threshold is not reached, there is no need to check corresponding {\it CacheTrackings}. 
Figure~\ref{fig:algorithm} illustrates the detailed mechanism.

\begin{figure}[!t]
\begin{lstlisting}
void HandleAccess(unsigned long addr, bool isWrite) {
 unsigned long cacheIndex=addr>>CACHELINE_SIZE_SHIFTS;
 cachetrack *track=NULL;

 if(CacheWrites[cacheIndex]<TRACKING_THRESHOLD) {
  if(isWrite) {
   if(ATOMIC_INCR(&CacheWrites[cacheIndex]) 
      ==TRACKING_THRESHOLD-1) {
    track=allocCacheTrack();
    ATOMIC_CAS(&CacheTrackings[cacheIndex],0,track));
   }
  } 
 }
 else {
  track=CacheTrackings[index]);
  if(track){
   // Track cache invalidations and detailed accesses
   track->handleAccess(addr, isWrite);
  }
 }
}
\end{lstlisting}
\caption{Pseudo-code to handle an access.\label{fig:algorithm}}
\end{figure}

To avoid expensive lock operations, \defaults{} uses atomic instruction to increment 
the counter of writes for each cache line. 
When the number of writes of a cache line reaches the predefined threshold,
it allocates space to track detailed cache invalidations and word accesses and then 
uses atomic compare-and-swap instruction to set cache tracking address for this cache line in
the shadow mapping. From now on, all accesses on this cache line are started to be tracked.

\subsection{Optimization}
\label{optimization}
Tracking memory accesses can be prohitively expensive. To improve 
performance, \defaults{} employs several mechanisms. This section discribes
them in detail.
%, where main improvement has been evaluated in Section~\ref{}.

\subsubsection{Selective Compiler Instrumetation}
\defaults{} relies on instrumentation to provide memory access information to the runtime system 
and detects false sharing based on memory traces. 
Thus the amount of instrumentation can largely affect the performance overhead: more 
instrumentation always means more performance overhead. 
Thus, \defaults{} provides a very flexible framework to instrument programs depending on performance requirements. 

% in a flexible fashion so that only necessary memory read/write accesses are 
%provided to runtime system in order to reduce performance overhead of detection. 
Currently, \Defaults{} only instruments once for one type of memory access on each address 
in one basic block. 
We argue that this sampling mechanism here do not affect the effectiveness of detection. 
This is because \Defaults{} targets to detect performance degrading false sharing with a large amount of cache invalidations,
and less tracking of one basic block can induce less cache invalidations but it won't affect the overall behavior of cache invalidations. 

% detection will not cause performance problem. 
Also, \defaults{} can be easily extended to support more selective instrumentation as follows:
\begin{itemize}
\item
\Defaults{} can selectively instrument reads-and-writes or writes-only. Fo example, instrumenting writes-only can help to detect write-write false sharing, as Sheriff does. 
\item
\Defaults{} can be set to instrument or skip one specific kind of targets. For example, user can provide a black list so that those modules,
functions or variables are not be isntrumented. Also, user can provide a red list so that only specified functions or variables are instrumented. 
\end{itemize}

\subsubsection{Sampling Mechanism}
Threadshold-based mechanism described above requires to maintain a counter for each
line. Updating those counters 
can exacerbate the performance. For instance, if originally a cache line exists false
sharing, invalidation of original cache line leads to another cache invalidation on
those cache lines containing the counter.

%To reduce the performance overhead caused by this, \defaults{} only samples the first specified
%number of accesses of each sampling interval for those those problematic cache lines. 
%Note that we originally maintains a global counter for all accesses and uses the number of 
%all accesses as a sample interval. However, using a global counter for all accesses creates 
%high performance overhead caused by cache invalidations. 

To reduce the performance overhead caused by this, \defaults{} only samples the first specified
number of accesses of each sampling interval for those those problematic cache lines. 
Currently, we maintain an access counter for each cache line and only samples the first $10000$ accesses 
for every 1 Million accesses (sampling interval), with sampling rate 1\%. By using this mechanism, 
we greatly reduce the performance overhead while not missing any performance-degrading false sharing.  

\begin{comment}
\subsubsection{Updating-In-Place}
%During the development of \defaults{}, we discover over $2\times$ performance overhead for those benchmarks 
%without any false sharing problems. 

Originally, all memory accesses are instrumented with a library call to notify the runtime system. However, this creates
some unnecessary performance overhead from function calls and library calls. 
A library call invokes normal function call overhead plus another indirection overhead through a Global Offset Table (GOT) and 
Procedure Linkage Table (PLT).
This can introduce significant performance overhead for some simple logic listed in the following:
\begin{itemize}
\item
When total writes on a cache line is less than the pre-defined threshold, if an access is a write, 
\defaults{} simply incrementes the counter of writes for this cache line. If an access is a read,
\defaults{} does not do  anything. 
\item
When total writes on a cache line is larger than the pre-defined threshold, then this cache line is under 
tracking. As described in last section, \defaults{} only sample certain number of accesses (1\%) for every 
sample interval. Most accesses (99\%) only needs to increment an access counter of corresponding cache line.
\end{itemize}

These simple logic only invokes several circles of useful work, either checking or updating a counter. 
Thus, the overhead of function calls or library
calls can be much more than this. 
In order to avoid these overhead, \defaults{} implements these simple logic in places where these
memory accesses are instrumented. 
This brings another problem: how to choose shadow mapping addresses. 
We borrow the idea of AddressSanitizer~\cite{Addresssanitizer} by 
choosing these addresses statically. \defaults{} pre-allocated the address space between $0x40000000$ and
$0xC0000000$ for the heap. Also, \defaults{} chooses $0x200000000000$ as the starting address for the shadow mapping of cache writes 
$0x200080000000$ as the starting address for the shadow mapping of cache tracking. 
Also, \defaults{} intentionally stores the access counter in the first word of cache tracking so that 
those checking and updating can be executed in-place when an memory access is instrumented. 
\end{comment}
